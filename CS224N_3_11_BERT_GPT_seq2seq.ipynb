{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from util import sequence_cross_entropy_with_logits\n",
    "import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertModel, BertConfig, AutoTokenizer, AutoModelForMaskedLM, AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "PATH_NAME = \"./\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sections of config\n",
    "\n",
    "# Defining key variables for dataLoader, Training\n",
    "MAX_LEN = 200\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-05\n",
    "\n",
    "bert_checkpoint = \"trueto/medbert-base-wwm-chinese\"\n",
    "encoder_tokenizer = AutoTokenizer.from_pretrained(bert_checkpoint)\n",
    "encoder_tokenizer.model_max_len=512\n",
    "EPOCHS=3\n",
    "FILE_NAME = \"3-14-seq-GPT.bin\"\n",
    "\n",
    "gpt_checkpoint = \"uer/gpt2-chinese-cluecorpussmall\"\n",
    "decoder_tokenizer = AutoTokenizer.from_pretrained(gpt_checkpoint)\n",
    "\n",
    "warmup_steps = 1e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>病人：脱发，杨医生你好，我妈妈三个月前开始发现脱发厉害，刚开始时掉头发，现在是连眉毛都开始有...</td>\n",
       "      <td>医生：可能是普秃，属于重症斑秃常，与神经、免疫和内分泌有关，应该查一下T细胞亚群、T3/T4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>病人：纤维腺瘤，这段时间来月经前就一直左乳比较涨痛。</td>\n",
       "      <td>医生：已诊。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>病人：便秘，便秘灌肠，四五天不大便，大便不干，发黑。</td>\n",
       "      <td>医生：你应该找找原因，吃中药调理。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>病人：最初大三阳现在是小三阳，hbsag420.1hbsab2.1hbeag0hbsab0....</td>\n",
       "      <td>医生：说明感染过乙肝病毒，应该检测肝功能、HBVDNA，同时。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>病人：牙痛，一个多月了，最早是不舒服，最近非常痛，痛起来连着左太阳穴一起痛，位置是左上里面第...</td>\n",
       "      <td>医生：根据症状判断应该是龋齿引起牙髓发炎，需要开髓做根管治疗。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  病人：脱发，杨医生你好，我妈妈三个月前开始发现脱发厉害，刚开始时掉头发，现在是连眉毛都开始有...   \n",
       "1                         病人：纤维腺瘤，这段时间来月经前就一直左乳比较涨痛。   \n",
       "2                         病人：便秘，便秘灌肠，四五天不大便，大便不干，发黑。   \n",
       "3  病人：最初大三阳现在是小三阳，hbsag420.1hbsab2.1hbeag0hbsab0....   \n",
       "4  病人：牙痛，一个多月了，最早是不舒服，最近非常痛，痛起来连着左太阳穴一起痛，位置是左上里面第...   \n",
       "\n",
       "                                                   1  \n",
       "0  医生：可能是普秃，属于重症斑秃常，与神经、免疫和内分泌有关，应该查一下T细胞亚群、T3/T4...  \n",
       "1                                             医生：已诊。  \n",
       "2                                  医生：你应该找找原因，吃中药调理。  \n",
       "3                    医生：说明感染过乙肝病毒，应该检测肝功能、HBVDNA，同时。  \n",
       "4                    医生：根据症状判断应该是龋齿引起牙髓发炎，需要开髓做根管治疗。  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "f = open('Dataset/validate_data.json')\n",
    "data = json.load(f)\n",
    "f.close()\n",
    "# pandas df works better than a list, so much faster wow\n",
    "df = pd.DataFrame(data)\n",
    "df = df.loc[:, :1]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, encoder_tokenizer, decoder_tokenizer, max_len):\n",
    "        self.encoder_tokenizer = encoder_tokenizer\n",
    "        self.decoder_tokenizer = decoder_tokenizer\n",
    "        self.dataframe = df\n",
    "        self.patient = df[0]\n",
    "        self.doc = df[1]\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # grab patient's utterance\n",
    "        input = str(self.patient[index])\n",
    "        input = \" \".join(input.split())\n",
    "\n",
    "        inputs = self.encoder_tokenizer.encode_plus(\n",
    "            input,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        input_ids = inputs['input_ids']\n",
    "        input_mask = inputs['attention_mask']\n",
    "        input_token_type_ids = inputs[\"token_type_ids\"]\n",
    "        \n",
    "        # grab doc's utterance\n",
    "        output = str(self.doc[index])\n",
    "        output = \" \".join(output.split())\n",
    "\n",
    "        outputs = self.decoder_tokenizer.encode_plus(\n",
    "            output,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        output_ids = outputs['input_ids']\n",
    "        output_mask = outputs['attention_mask']\n",
    "        output_token_type_ids = outputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'input_mask': torch.tensor(input_mask, dtype=torch.long),\n",
    "            'input_token_type_ids': torch.tensor(input_token_type_ids, dtype=torch.long),\n",
    "            'output_ids': torch.tensor(output_ids, dtype=torch.long),\n",
    "            'output_mask': torch.tensor(output_mask, dtype=torch.long),\n",
    "            'output_token_type_ids': torch.tensor(output_token_type_ids, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: 340749\n",
      "TRAIN Dataset: 272599\n",
      "TEST Dataset: 68150\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset and dataloader for the neural network\n",
    "train_size = 0.8\n",
    "train_dataset=df.sample(frac=train_size,random_state=200)\n",
    "\n",
    "test_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(len(data)))\n",
    "print(\"TRAIN Dataset: {}\".format(len(train_dataset)))\n",
    "print(\"TEST Dataset: {}\".format(len(test_dataset)))\n",
    "\n",
    "training_set = CustomDataset(train_dataset, encoder_tokenizer, decoder_tokenizer, MAX_LEN)\n",
    "testing_set = CustomDataset(test_dataset, encoder_tokenizer, decoder_tokenizer, MAX_LEN)\n",
    "\n",
    "train_params = {'batch_size': BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 2\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 2\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$BERT + GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at trueto/medbert-base-wwm-chinese were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at uer/gpt2-chinese-cluecorpussmall and are newly initialized: ['transformer.h.5.crossattention.c_attn.weight', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.1.crossattention.bias', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.10.crossattention.bias', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.5.crossattention.masked_bias', 'transformer.h.2.crossattention.masked_bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.0.crossattention.masked_bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.7.crossattention.bias', 'transformer.h.0.crossattention.bias', 'transformer.h.2.crossattention.bias', 'transformer.h.3.crossattention.masked_bias', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.9.crossattention.masked_bias', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.6.crossattention.masked_bias', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.7.crossattention.masked_bias', 'transformer.h.4.crossattention.bias', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.8.crossattention.masked_bias', 'transformer.h.4.crossattention.masked_bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.6.crossattention.bias', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.3.crossattention.bias', 'transformer.h.10.crossattention.masked_bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.11.crossattention.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.11.crossattention.masked_bias', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.5.crossattention.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.8.crossattention.bias', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.9.ln_cross_attn.weight', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.1.crossattention.masked_bias', 'transformer.h.9.crossattention.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from transformers import DataCollatorWithPadding,AutoModelForSequenceClassification, Trainer, TrainingArguments,AutoTokenizer,AutoModel,AutoConfig, BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline, GPT2Config, AdamW\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "class CustomModel(torch.nn.Module):\n",
    "  def __init__(self,checkpoint,num_labels,temperature=0.5, dropout_rate = 0.1): \n",
    "    super(CustomModel,self).__init__() \n",
    "    self.num_labels = num_labels \n",
    "    self.temperature = temperature\n",
    "    self.dropout_rate = dropout_rate\n",
    "\n",
    "    #Load Model with given checkpoint and extract its body\n",
    "    myConfig = AutoConfig.from_pretrained(checkpoint,output_hidden_states=True)\n",
    "    myConfig.problem_type = \"multi_label_classification\"\n",
    "    myConfig.temperature = self.temperature\n",
    "    myConfig.output_attentions = True\n",
    "\n",
    "    self.encoder = AutoModel.from_pretrained(checkpoint,config=myConfig)\n",
    "    self.decoder = GPT2LMHeadModel.from_pretrained(gpt_checkpoint, add_cross_attention=True).to(device)\n",
    "\n",
    "    self.dropout = torch.nn.Dropout(self.dropout_rate) \n",
    "    self.classifier = torch.nn.Linear(self.encoder.config.hidden_size,num_labels) # load and initialize weights\n",
    "    self.criterion = torch.nn.CrossEntropyLoss() # define loss function\n",
    "\n",
    "  def forward(self, encoder_input_ids=None, encoder_attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None,labels=None):\n",
    "    #Extract outputs from the body\n",
    "\n",
    "    outputs = self.encoder(input_ids=encoder_input_ids, attention_mask=encoder_attention_mask, output_hidden_states=True)\n",
    "    # select the 12th layer \n",
    "    hidden_states = outputs.hidden_states[-1]\n",
    "\n",
    "    # this decoder outputs a dict of loss, logits, past_key_values, hidden_states, attentions, cross_attentions\n",
    "    seqoutputs = self.decoder(input_ids=decoder_input_ids, attention_mask=decoder_attention_mask, encoder_hidden_states = hidden_states, labels=decoder_input_ids) \n",
    "    \n",
    "    return seqoutputs\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=CustomModel(checkpoint=bert_checkpoint,num_labels=10).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(optimizer, model, training_loader, testing_loader, device, num_epochs, learning_rate = 0.1):\n",
    "#------------------------START TRAINING-------------------\n",
    "    update_count = 0\n",
    "\n",
    "    start = time.time()\n",
    "    print('start training....')\n",
    "    for epoch in range(num_epochs):\n",
    "        #------------------------training------------------------\n",
    "        model.train()\n",
    "        total_losses = 0\n",
    "        seq_losses = 0\n",
    "        times = 0\n",
    "        for _, data in enumerate(training_loader, 0):\n",
    "            encoder_input = data['input_ids'].to(device, dtype = torch.long)\n",
    "            mask_encoder_input = data['input_mask'].to(device, dtype = torch.long)\n",
    "            #encoder_token_type_ids = data['input_token_type_ids'].to(device, dtype = torch.long)\n",
    "            decoder_input = data['output_ids'].to(device, dtype = torch.long)\n",
    "            mask_decoder_input = data['output_mask'].to(device, dtype=torch.long)\n",
    "            #decoder_token_type_ids = data['output_token_type_ids'].to(device, dtype=torch.long)\n",
    "\n",
    "\n",
    "            outputs= model(encoder_input, mask_encoder_input, decoder_input, mask_decoder_input)\n",
    "            \n",
    "            seq_loss = outputs.loss\n",
    "            seq_loss.backward()\n",
    "\n",
    "            total_losses += seq_loss.item()\n",
    "            times += 1\n",
    "            update_count += 1\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        end = time.time()\n",
    "        print('-'*20 + f'epoch {epoch}' + '-'*20)\n",
    "        print(f'time: {(end - start)}')\n",
    "        print(f'total loss: {total_losses / times}')\n",
    "        start = end\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_training_steps = EPOCHS * len(training_loader)\n",
    "#progress_bar_train = tqdm(range(num_training_steps))\n",
    "\n",
    "# call train function\n",
    "#train(optimizer, model, testing_loader, testing_loader, device, EPOCHS, LEARNING_RATE)\n",
    "\n",
    "# save the model after training\n",
    "#print('----- saving model -----')\n",
    "#torch.save(model.state_dict(), \"{PATH_NAME}/bert_gpt_validate_data_testingloader_seq2seq.bin\".format(PATH_NAME=PATH_NAME))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(\n",
    "    batch_size=1,\n",
    "    decoder_path='decoder.pth',\n",
    "    model = None,\n",
    "    test_dataloader=None,\n",
    "    device='cuda'):\n",
    "    # make sure your model is on GPU\n",
    "    device = torch.device(device)\n",
    "\n",
    "    #------------------------LOAD MODEL-----------------\n",
    "    print('load the model....')\n",
    "    model = model\n",
    "    model.load_state_dict(torch.load(decoder_path))\n",
    "    print(f'load from {decoder_path}')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    print('load success')\n",
    "    #------------------------END LOAD MODEL--------------\n",
    "    #------------------------END LOAD VAL DATA--------------\n",
    "\n",
    "    perplexity = 0\n",
    "    batch_count = 0\n",
    "    print('start calculate the test perplexity....')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(test_dataloader, 0):\n",
    "            encoder_input = data['input_ids'].to(device, dtype = torch.long)\n",
    "            mask_encoder_input = data['input_mask'].to(device, dtype = torch.long)\n",
    "            decoder_input = data['output_ids'].to(device, dtype = torch.long)\n",
    "            mask_decoder_input = data['output_mask'].to(device, dtype=torch.long)\n",
    "\n",
    "            outputs = model(encoder_input, mask_encoder_input, decoder_input, mask_decoder_input)\n",
    "\n",
    "            logits = outputs.logits\n",
    "            out = logits[:, :-1].contiguous()\n",
    "            target = decoder_input[:, 1:].contiguous()\n",
    "            target_mask = mask_decoder_input[:, 1:].contiguous()\n",
    "\n",
    "            loss = sequence_cross_entropy_with_logits(out, target, target_mask, average=\"token\")\n",
    "            perplexity += np.exp(loss.item())\n",
    "            batch_count += 1\n",
    "\n",
    "\n",
    "    print(f'test perplexity: {perplexity / batch_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing in the testing set, get perplexity values on training \n",
    "#calculate_perplexity(batch_size=BATCH_SIZE, decoder_path=\"{PATH_NAME}/bert_gpt_validate_data_testingloader_seq2seq.bin\".format(PATH_NAME=PATH_NAME), model=model, test_dataloader=testing_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/ubuntu/.local/lib/python3.8/site-packages (3.8.1)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/.local/lib/python3.8/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: joblib in /home/ubuntu/.local/lib/python3.8/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (7.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "from collections import defaultdict\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.translate.nist_score import sentence_nist\n",
    "from nltk.util import ngrams\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def bleu(predict, target, n):\n",
    "    return sentence_bleu([target], predict, weights=tuple(1 / n for i in range(n)))\n",
    "\n",
    "\n",
    "def nist(predict, target, n):\n",
    "    if len(predict) < n or len(target) < n:\n",
    "        return 0\n",
    "    return sentence_nist([target], predict, n)\n",
    "\n",
    "\n",
    "def cal_entropy(generated):\n",
    "    etp_score = [0.0, 0.0, 0.0, 0.0]\n",
    "    div_score = [0.0, 0.0, 0.0, 0.0]\n",
    "    counter = [defaultdict(int), defaultdict(int),\n",
    "               defaultdict(int), defaultdict(int)]\n",
    "    for gg in generated:\n",
    "        g = gg.rstrip().split()\n",
    "        for n in range(4):\n",
    "            for idx in range(len(g)-n):\n",
    "                ngram = ' '.join(g[idx:idx+n+1])\n",
    "                counter[n][ngram] += 1\n",
    "    for n in range(4):\n",
    "        total = sum(counter[n].values()) + 1e-10\n",
    "        for v in counter[n].values():\n",
    "            etp_score[n] += - (v+0.0) / total * (np.log(v+0.0) - np.log(total))\n",
    "        div_score[n] = (len(counter[n].values())+0.0) / total\n",
    "    return etp_score, div_score\n",
    "\n",
    "\n",
    "def cal_length(sentences):\n",
    "    sen_length = [len(s.split()) for s in sentences]\n",
    "    return np.mean(sen_length), np.var(sen_length)\n",
    "\n",
    "\n",
    "def calculate_metrics(predict, reference):\n",
    "    reference_len = len(reference)\n",
    "    predict_len = len(predict)\n",
    "\n",
    "    #-------------------bleu----------\n",
    "    bleu_2 = bleu(predict, reference, 2)\n",
    "    bleu_4 = bleu(predict, reference, 4)\n",
    "    #-------------------nist----------\n",
    "    nist_2 = nist(predict, reference, 2)\n",
    "    nist_4 = nist(predict, reference, 4)\n",
    "    #-------------------meteor----------\n",
    "    predict = \" \".join(predict)\n",
    "    reference = \" \".join(reference)\n",
    "    meteor_scores = meteor_score([reference], predict)\n",
    "    return bleu_2, bleu_4, nist_2, nist_4, meteor_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from metrics import cal_entropy, cal_length, calculate_metrics\n",
    "\n",
    "\n",
    "def validation(file_name):\n",
    "    with open(file_name, \"r\", encoding='utf-8') as f:\n",
    "        json_data = f.read()\n",
    "        data = json.loads(json_data)\n",
    "    \n",
    "    bleu_2scores = 0\n",
    "    bleu_4scores = 0\n",
    "    nist_2scores = 0\n",
    "    nist_4scores = 0\n",
    "    meteor_scores = 0\n",
    "    sentences = []\n",
    "\n",
    "    for d in tqdm(data):\n",
    "        reference = list(d['reference'])\n",
    "        predict = list(d['predict'])\n",
    "        temp_bleu_2, \\\n",
    "        temp_bleu_4, \\\n",
    "        temp_nist_2, \\\n",
    "        temp_nist_4, \\\n",
    "        temp_meteor_scores = calculate_metrics(predict, reference)\n",
    "\n",
    "        bleu_2scores += temp_bleu_2\n",
    "        bleu_4scores += temp_bleu_4\n",
    "        nist_2scores += temp_nist_2\n",
    "        nist_4scores += temp_nist_4\n",
    "        meteor_scores += temp_meteor_scores\n",
    "        sentences.append(\" \".join(predict))\n",
    "\n",
    "    entro, dist = cal_entropy(sentences)\n",
    "    mean_len, var_len = cal_length(sentences)\n",
    "    num = len(sentences)\n",
    "    print(f'avg: {mean_len}, var: {var_len}')\n",
    "    print(f'entro: {entro}')\n",
    "    print(f'dist: {dist}')\n",
    "    print(f'bleu_2scores: {bleu_2scores / num}')\n",
    "    print(f'bleu_4scores: {bleu_4scores / num}')\n",
    "    print(f'nist_2scores: {nist_2scores / num}')\n",
    "    print(f'nist_4scores: {nist_4scores / num}')\n",
    "    print(f'meteor_scores: {meteor_scores / num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def top_k_logits(logits, k):\n",
    "    \"\"\"Mask logits so that only top-k logits remain\n",
    "    \"\"\"\n",
    "    values, _ = torch.topk(logits, k)\n",
    "    print('logits.shape', logits.shape)\n",
    "    print('values.shape', values.shape)\n",
    "    print('values[:,-1] shape', values[:,-1].shape)\n",
    "    print('values[:, :, -1].unsqueeze(0)', values[:, -1].unsqueeze(1).shape)\n",
    "    min_values = values[:, -1].unsqueeze(1).repeat(1, logits.shape[-1], 4)\n",
    "    min_values = min_values.transpose(1, 2)\n",
    "    #min_values = values[:, :, -1].unsqueeze(0).repeat(1, 1, logits.shape[-1])\n",
    "    print('min_values,shape', min_values.shape)\n",
    "    return torch.where(logits < min_values, torch.ones_like(logits, dtype=logits.dtype) * -1e10, logits)\n",
    "\n",
    "def get_decoder(decoder_path):\n",
    "    old_state_dict = torch.load(decoder_path, map_location='cuda')\n",
    "    print(f'load from {decoder_path}')\n",
    "    model = CustomModel(checkpoint=bert_checkpoint,num_labels=10).to(device)\n",
    "    encoder = model.encoder\n",
    "    decoder = model.decoder\n",
    "\n",
    "    encoder_state_dict = encoder.state_dict()\n",
    "    for i in encoder_state_dict.keys():\n",
    "        encoder_state_dict[i] = old_state_dict['encoder.' + i]\n",
    "    encoder.load_state_dict(encoder_state_dict)\n",
    "\n",
    "    decoder_state_dict = decoder.state_dict()\n",
    "    for i in decoder_state_dict.keys():\n",
    "        decoder_state_dict[i] = old_state_dict['decoder.' + i]\n",
    "    decoder.load_state_dict(decoder_state_dict)\n",
    "    return encoder, decoder\n",
    "\n",
    "def generate_sentences(test_dataloader, encoder_tokenizer, decoder_tokenizer, decoder_path, top_k, l):\n",
    "    # make sure your model is on GPU\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "    #------------------------LOAD MODEL-----------------\n",
    "    encoder, decoder = get_decoder(decoder_path)\n",
    "    encoder = encoder.to(device)\n",
    "    decoder = decoder.to(device)\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    #------------------------END LOAD VALIDATE DATA--------------\n",
    "\n",
    "\n",
    "    #------------------------START SAMPLE GENERETE-------------------\n",
    "    for _, data in enumerate(test_dataloader, 0):\n",
    "        with torch.no_grad():\n",
    "            encoder_input = data['input_ids'].to(device, dtype = torch.long)\n",
    "            mask_encoder_input = data['input_mask'].to(device, dtype = torch.long)\n",
    "            decoder_input = data['output_ids'].to(device, dtype = torch.long)\n",
    "            mask_decoder_input = data['output_mask'].to(device, dtype=torch.long)\n",
    "            encoder_outputs = encoder(input_ids=encoder_input, attention_mask=mask_encoder_input, output_hidden_states=True)\n",
    "            hidden_states = encoder_outputs.hidden_states[-1]\n",
    "            sentence = []\n",
    "\n",
    "            prev_pred = decoder_input[:, :1]\n",
    "            sentence.append(prev_pred)\n",
    "\n",
    "            length = 1\n",
    "            # decoding loop\n",
    "            for i in range(100):\n",
    "                # this decoder outputs a dict of loss, logits, past_key_values, hidden_states, attentions, cross_attentions\n",
    "                seqoutputs = decoder(input_ids=decoder_input, attention_mask=mask_decoder_input, encoder_hidden_states = hidden_states, labels=decoder_input) \n",
    "                \n",
    "                logits = seqoutputs.logits.squeeze(1)\n",
    "                logits = top_k_logits(logits, k=top_k)\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                prev_pred = torch.multinomial(probs, num_samples=1)\n",
    "                sentence.append(prev_pred)\n",
    "                if prev_pred[0][0] == 102:\n",
    "                    break\n",
    "                length += 1\n",
    "\n",
    "            sentence = torch.cat(sentence, dim=-1)\n",
    "            predict = decoder_tokenizer.convert_ids_to_tokens(sentence[0].tolist())\n",
    "\n",
    "            target = decoder_input.squeeze(dim=0)\n",
    "            target_num = (target != 0).sum()\n",
    "            reference = encoder_tokenizer.convert_ids_to_tokens(target[:target_num].tolist())\n",
    "\n",
    "            encoder_input = encoder_input.squeeze(dim=0)\n",
    "            encoder_input_num = (encoder_input != 0).sum()\n",
    "            inputs = decoder_tokenizer.convert_ids_to_tokens(encoder_input[:encoder_input_num].tolist())\n",
    "            l.append([\"\".join(inputs[1:-1]), \"\".join(predict[1:-1]), \"\".join(reference[1:-1])])\n",
    "\n",
    "\n",
    "    #------------------------END SAMPLE GENERETE-------------------\n",
    "\n",
    "\n",
    "def sample_generate(\n",
    "    generate_filename,\n",
    "    top_k = 50,\n",
    "    decoder_path='/home/ubuntu/CS_224N/bert_gpt_validate_data_testingloader_seq2seq.bin',\n",
    "    process_num=1\n",
    "    ):\n",
    "    encoder_tokenizer = AutoTokenizer.from_pretrained(bert_checkpoint)\n",
    "    decoder_tokenizer = AutoTokenizer.from_pretrained(gpt_checkpoint)\n",
    "    l=list()\n",
    "    testing_loader = DataLoader(testing_set, **test_params)\n",
    "    generate_sentences(test_dataloader=testing_loader, encoder_tokenizer=encoder_tokenizer, decoder_tokenizer=decoder_tokenizer, decoder_path=decoder_path, top_k=top_k, l=l)\n",
    "    \n",
    "    \"\"\"\n",
    "    mgr = mp.Manager()\n",
    "    l = mgr.list()\n",
    "    processes = []\n",
    "    for rank in range(process_num):\n",
    "        if rank == process_num - 1:\n",
    "            ## handles the last batch of data\n",
    "            data = test_data.iloc[int((rank / process_num) * length):]\n",
    "        else:\n",
    "            data = test_data.iloc[int((rank / process_num) * length) : int(((rank + 1) / process_num) * length)]\n",
    "            print('length of batched_data', len(data))\n",
    "        batch_data_loader = CustomDataset(data, encoder_tokenizer, decoder_tokenizer, MAX_LEN)\n",
    "        p = mp.Process(target=generate_sentences, args=(batch_data_loader, tokenizer, decoder_path, rank, top_k, l))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "    \"\"\"\n",
    "\n",
    "    Dialog_list = []\n",
    "    with open(generate_filename, 'w', encoding='utf-8') as f:\n",
    "        for s in l:\n",
    "            cases = dict()\n",
    "            cases['input'] = s[0]\n",
    "            cases['predict'] = s[1]\n",
    "            cases['reference'] = s[2]\n",
    "            Dialog_list.append(cases)\n",
    "        json.dump(Dialog_list, f, ensure_ascii = False, indent = 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 22.20 GiB total capacity; 18.99 GiB already allocated; 18.06 MiB free; 20.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sample_generate(generate_filename\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgenerate_sentences.txt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mfinished generating\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m validation(\u001b[39m'\u001b[39m\u001b[39mgenerate_sentences.txt\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[37], line 105\u001b[0m, in \u001b[0;36msample_generate\u001b[0;34m(generate_filename, top_k, decoder_path, process_num)\u001b[0m\n\u001b[1;32m    103\u001b[0m l\u001b[39m=\u001b[39m\u001b[39mlist\u001b[39m()\n\u001b[1;32m    104\u001b[0m testing_loader \u001b[39m=\u001b[39m DataLoader(testing_set, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtest_params)\n\u001b[0;32m--> 105\u001b[0m generate_sentences(test_dataloader\u001b[39m=\u001b[39;49mtesting_loader, encoder_tokenizer\u001b[39m=\u001b[39;49mencoder_tokenizer, decoder_tokenizer\u001b[39m=\u001b[39;49mdecoder_tokenizer, decoder_path\u001b[39m=\u001b[39;49mdecoder_path, top_k\u001b[39m=\u001b[39;49mtop_k, l\u001b[39m=\u001b[39;49ml)\n\u001b[1;32m    107\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39mmgr = mp.Manager()\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39ml = mgr.list()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39m    p.join()\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    126\u001b[0m Dialog_list \u001b[39m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[37], line 41\u001b[0m, in \u001b[0;36mgenerate_sentences\u001b[0;34m(test_dataloader, encoder_tokenizer, decoder_tokenizer, decoder_path, top_k, l)\u001b[0m\n\u001b[1;32m     38\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[39m#------------------------LOAD MODEL-----------------\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m encoder, decoder \u001b[39m=\u001b[39m get_decoder(decoder_path)\n\u001b[1;32m     42\u001b[0m encoder \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     43\u001b[0m decoder \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[37], line 19\u001b[0m, in \u001b[0;36mget_decoder\u001b[0;34m(decoder_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_decoder\u001b[39m(decoder_path):\n\u001b[0;32m---> 19\u001b[0m     old_state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(decoder_path, map_location\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     20\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mload from \u001b[39m\u001b[39m{\u001b[39;00mdecoder_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m     model \u001b[39m=\u001b[39m CustomModel(checkpoint\u001b[39m=\u001b[39mbert_checkpoint,num_labels\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:712\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m             opened_file\u001b[39m.\u001b[39mseek(orig_position)\n\u001b[1;32m    711\u001b[0m             \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 712\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    713\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:1046\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1044\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1045\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1046\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1048\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1050\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:1016\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m loaded_storages:\n\u001b[1;32m   1015\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1016\u001b[0m     load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   1018\u001b[0m \u001b[39mreturn\u001b[39;00m loaded_storages[key]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:1001\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m    997\u001b[0m storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39m_UntypedStorage)\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39m_untyped()\n\u001b[1;32m    998\u001b[0m \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m    999\u001b[0m \u001b[39m# stop wrapping with _TypedStorage\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m loaded_storages[key] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39m_TypedStorage(\n\u001b[0;32m-> 1001\u001b[0m     wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1002\u001b[0m     dtype\u001b[39m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:970\u001b[0m, in \u001b[0;36m_get_restore_location.<locals>.restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrestore_location\u001b[39m(storage, location):\n\u001b[0;32m--> 970\u001b[0m     \u001b[39mreturn\u001b[39;00m default_restore_location(storage, map_location)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:176\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    175\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 176\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[1;32m    177\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:158\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[39mreturn\u001b[39;00m storage_type(obj\u001b[39m.\u001b[39mnbytes())\n\u001b[1;32m    157\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39;49mcuda(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_utils.py:79\u001b[0m, in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     new_type \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(torch\u001b[39m.\u001b[39mcuda, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m---> 79\u001b[0m     \u001b[39mreturn\u001b[39;00m new_type(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize())\u001b[39m.\u001b[39mcopy_(\u001b[39mself\u001b[39m, non_blocking)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:661\u001b[0m, in \u001b[0;36m_lazy_new\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m _lazy_init()\n\u001b[1;32m    659\u001b[0m \u001b[39m# We may need to call lazy init again if we are a forked child\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[39m# del _CudaBase.__new__\u001b[39;00m\n\u001b[0;32m--> 661\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(_CudaBase, \u001b[39mcls\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__new__\u001b[39;49m(\u001b[39mcls\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 22.20 GiB total capacity; 18.99 GiB already allocated; 18.06 MiB free; 20.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "sample_generate(generate_filename='generate_sentences.txt')\n",
    "print('finished generating')\n",
    "validation('generate_sentences.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
